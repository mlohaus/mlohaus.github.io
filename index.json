[{"authors":null,"categories":null,"content":"NEWS My PhD is over and I started looking for jobs. Feel free to contact me.\nNEWS Paper accepted at NeurIPS 2022. Read it here!\nCurrently, the main topic of my PhD is fairness in machine learning which considers the impact of automated decision-making on human lifes. In order to make sure good decisions are made, everyone applies some notion of fairness to a large-scale decision-making system, in particular, when these decisions change people\u0026rsquo;s lives. I study decision-making systems, which aim to be fair to groups of individuals, and I focus on investigating their fairness properties.\nI have also studied Comparison-based learning, where we are not given representations of data points, but we know for three items A, B, C if item A is closer to item B or closer to item C.\nIn October 2017, I started my PhD in the Theory of Machine Learning group (TML) supervised by Prof.Dr. Ulrike von Luxburg at the University of Tübingen. I am a scholar in the International Max Planck Research School for Intelligent Systems (IMPRS-IS), a graduate school by both the universities and Max-Planck-Institutes in Tuebingen and Stuttgart.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1664023165,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://mlohaus.github.io/author/michael-lohaus/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/michael-lohaus/","section":"authors","summary":"NEWS My PhD is over and I started looking for jobs. Feel free to contact me.\nNEWS Paper accepted at NeurIPS 2022. Read it here!\nCurrently, the main topic of my PhD is fairness in machine learning which considers the impact of automated decision-making on human lifes.","tags":null,"title":"Michael Lohaus","type":"authors"},{"authors":["Michael Lohaus","Matthäus Kleindessner","Krishnaram Kenthapadi","Francesco Locatello","Chris Russell"],"categories":null,"content":"","date":1663459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664023536,"objectID":"86dc3e4264adced9c8da6dfcbba2926d","permalink":"https://mlohaus.github.io/publication/disparatetreatment/","publishdate":"2022-09-18T00:00:00Z","relpermalink":"/publication/disparatetreatment/","section":"publication","summary":"We show that deep neural networks that satisfy demographic parity do so through a form of race or gender awareness, and that the more we force a network to be fair, the more accurately we can recover race or gender from the internal state of the network. Based on this observation, we propose a simple two-stage solution for enforcing fairness. First, we train a two-headed network to predict the protected attribute (such as race or gender) alongside the original task, and second, we enforce demographic parity by taking a weighted sum of the heads. In the end, this approach creates a single-headed network with the same backbone architecture as the original network. Our approach has near identical performance compared to existing regularization-based or preprocessing methods, but has greater stability and higher accuracy where near exact demographic parity is required. To cement the relationship between these two approaches, we show that an unfair and optimally accurate classifier can be recovered by taking a weighted sum of a fair classifier and a classifier predicting the protected attribute. We use this to argue that both the fairness approaches and our explicit formulation demonstrate disparate treatment and that, consequentially, they are likely to be unlawful in a wide range of scenarios under the US law.","tags":["Computer Vision","Fairness in Machine Learning"],"title":"Are Two Heads the Same as One? Identifying Disparate Treatment in Fair Neural Networks","type":"publication"},{"authors":["Dominik Zietlow","Michael Lohaus","Guha Balakrishnan","Matthäus Kleindessner","Francesco Locatello","Bernhard Schölkopf","Chris Russell"],"categories":null,"content":"","date":1648684800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664023536,"objectID":"a17ca1e39874b953841f7ee36d52f71f","permalink":"https://mlohaus.github.io/publication/levelingdown/","publishdate":"2022-06-21T00:00:00Z","relpermalink":"/publication/levelingdown/","section":"publication","summary":"Algorithmic fairness is frequently motivated in terms of a trade-off in which overall performance is decreased so as to improve performance on disadvantaged groups where the algorithm would otherwise be less accurate. Contrary to this, we find that applying existing fairness approaches to computer vision improve fairness by degrading the performance of classifiers across all groups (with increased degradation on the best performing groups). Extending the bias-variance decomposition for classification to fairness, we theoretically explain why the majority of fairness classifiers designed for low capacity models should not be used in settings involving high-capacity models, a scenario common to computer vision. We corroborate this analysis with extensive experimental support that shows that many of the fairness heuristics used in computer vision also degrade performance on the most disadvantaged groups. Building on these insights, we propose an adaptive augmentation strategy that, uniquely, of all methods tested, improves performance for the disadvantaged groups.","tags":["Computer Vision","Fairness in Machine Learning"],"title":"Leveling Down in Computer Vision: Pareto Inefficiencies in Fair Deep Classifiers","type":"publication"},{"authors":["Leena C Vankadara*","Michael Lohaus*","Siavash Haghiri","Faiz Ul Wahab","Ulrike von Luxburg"],"categories":null,"content":"","date":1634774400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664023165,"objectID":"2a86cf77c75ced00deb54a6cfb1a7fc9","permalink":"https://mlohaus.github.io/publication/insightsintoordinalembeddingalgorithms/","publishdate":"2021-10-21T00:00:00Z","relpermalink":"/publication/insightsintoordinalembeddingalgorithms/","section":"publication","summary":"The objective of ordinal embedding is to find a Euclidean representation of a set of abstract items, using only answers to triplet comparisons of the form \"Is item i closer to the item j or item k?\". In recent years, numerous algorithms have been proposed to solve this problem. However, there does not exist a fair and thorough assessment of these embedding methods and therefore several key questions remain unanswered: Which algorithms scale better with increasing sample size or dimension?  Which ones perform better when the embedding dimension is small or few triplet comparisons are available? In our paper, we address these questions and provide the first comprehensive and systematic empirical evaluation of existing algorithms as well as a new neural network approach. In the large triplet regime, we find that simple, relatively unknown, non-convex methods consistently outperform all other algorithms, including elaborate approaches based on neural networks or landmark approaches. This finding can be explained by our insight that many of the non-convex optimization approaches do not suffer from local optima. In the low triplet regime, our neural network approach is either competitive or significantly outperforms all the other methods. Our comprehensive assessment is enabled by our unified library of popular embedding algorithms that leverages GPU resources and allows for fast and accurate embeddings of millions of data points.","tags":["Comparison-Based Machine Learning","Ordinal Embeddings"],"title":"Insights into Ordinal Embedding Algorithms: A Systematic Evaluation","type":"publication"},{"authors":["Michael Lohaus","Michaël Perrot","Ulrike von Luxburg"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664023165,"objectID":"e8bf1dd118fa2683d7286a7ec1bfd7bf","permalink":"https://mlohaus.github.io/publication/toorelaxedtobefair/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/toorelaxedtobefair/","section":"publication","summary":"We address the problem of classification under fairness constraints. Given a notion of fairness, the goal is to learn a classifier that is not discriminatory against a group of individuals. In the literature, this problem is often formulated as a constrained optimization problem and solved using relaxations of the fairness constraints. We show that many existing relaxations are unsatisfactory: even if a model satisfies the relaxed constraint, it can be surprisingly unfair. We propose a principled framework to solve this problem. This new approach uses a strongly convex formulation and comes with theoretical guarantees on the fairness of its solution. In practice, we show that this method gives promising results on real data.","tags":["Fairness in Machine Learning","Fair Binary Classification"],"title":"Too Relaxed to Be Fair","type":"publication"},{"authors":["Michael Lohaus","Philipp Hennig","Ulrike von Luxburg"],"categories":null,"content":"","date":1561593600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664023165,"objectID":"6c07eddfb458d24e253f832ed356af3d","permalink":"https://mlohaus.github.io/publication/uncertaintyforordinalembeddings/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/uncertaintyforordinalembeddings/","section":"publication","summary":"To investigate objects without a describable notion of distance, one can gather ordinal information by asking triplet comparisons of the form \"Is object x closer to y or is x closer to z?\" In order to learn from such data, the objects are typically embedded in a Euclidean space while satisfying as many triplet comparisons as possible. In this paper, we introduce empirical uncertainty estimates for standard embedding algorithms when few noisy triplets are available, using a bootstrap and a Bayesian approach. In particular, simulations show that these estimates are well calibrated and can serve to select embedding parameters or to quantify uncertainty in scientific applications.","tags":["Comparison-Based Machine Learning","Ordinal Embeddings"],"title":"Uncertainty Estimates for Ordinal Embeddings","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot;\rif porridge == \u0026quot;blueberry\u0026quot;:\rprint(\u0026quot;Eating...\u0026quot;)\r  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}}\r{{% fragment %}} **Two** {{% /fragment %}}\r{{% fragment %}} Three {{% /fragment %}}\r Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}}\r- Only the speaker can read these notes\r- Press `S` key to view\r{{% /speaker_note %}}\r Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}}\r{{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}}\r{{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}\r  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1,\r.reveal section h2,\r.reveal section h3 {\rcolor: navy;\r}\r  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605467251,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://mlohaus.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"}]